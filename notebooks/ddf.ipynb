{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Distance Function Experiments\n",
    "This notebooks contains experiments in using a directional distance field for visibility in RENI-NeuS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the cwd to the root of the repo\n",
    "\n",
    "import os\n",
    "# change dir to root of repo\n",
    "os.chdir('..')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.cameras.rays import RayBundle\n",
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "from nerfstudio.field_components.encodings import SHEncoding, NeRFEncoding\n",
    "import tinycudann as tcnn\n",
    "\n",
    "from reni_neus.reni_neus_model import RENINeuSFactoModelConfig, RENINeuSFactoModel\n",
    "from reni_neus.utils.utils import get_directions, get_sineweight\n",
    "from reni_neus.illumination_fields.reni_field import RENIField\n",
    "\n",
    "def make_ray_bundle_clone(ray_bundle):\n",
    "    new_ray_bundle = RayBundle(\n",
    "      origins=ray_bundle.origins.detach().clone(),\n",
    "      directions=ray_bundle.directions.detach().clone(),\n",
    "      pixel_area=ray_bundle.pixel_area.detach().clone(),\n",
    "      directions_norm=ray_bundle.directions_norm.detach().clone(),\n",
    "      camera_indices=ray_bundle.camera_indices.detach().clone(),\n",
    "      nears=ray_bundle.nears.detach().clone() if ray_bundle.nears is not None else None,\n",
    "      fars=ray_bundle.fars.detach().clone() if ray_bundle.fars is not None else None,\n",
    "    )\n",
    "    return new_ray_bundle\n",
    "\n",
    "def make_batch_clone(batch):\n",
    "    new_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            new_batch[key] = value.detach().clone()\n",
    "        else:\n",
    "            new_batch[key] = value\n",
    "    return new_batch       \n",
    "\n",
    "def sRGB(imgs):\n",
    "    # Add batch dimension if necessary\n",
    "    if imgs.ndim == 3:\n",
    "        imgs = imgs.unsqueeze(0)\n",
    "    \n",
    "    # Calculate the 98th percentile for each image\n",
    "    q = torch.quantile(imgs.view(imgs.size(0), -1), 0.98, dim=1)\n",
    "    \n",
    "    # Normalize images by their 98th percentile\n",
    "    imgs = imgs / q.view(-1, 1, 1, 1)\n",
    "    \n",
    "    # Clamp the pixel values between 0.0 and 1.0\n",
    "    imgs = torch.clamp(imgs, 0.0, 1.0)\n",
    "    \n",
    "    # Convert linear RGB to sRGB using the sRGB conversion formula\n",
    "    mask = imgs <= 0.0031308\n",
    "    imgs_sRGB = imgs = torch.where(\n",
    "        imgs <= 0.0031308,\n",
    "        12.92 * imgs,\n",
    "        1.055 * torch.pow(torch.abs(imgs), 1 / 2.4) - 0.055,\n",
    "    )\n",
    "    return imgs_sRGB\n",
    "\n",
    "def rotation_matrix(axis, angle):\n",
    "    \"\"\"\n",
    "    Return 3D rotation matrix for rotating around the given axis by the given angle.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(angle / 2.0)\n",
    "    b, c, d = -axis * np.sin(angle / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "\n",
    "# setup config\n",
    "test_mode = 'val'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "ckpt_path = 'outputs/data-NeRF-OSR-Data/RENI-NeuS/latest_with_rot_and_clip_illumination/'\n",
    "step = 80000\n",
    "\n",
    "ckpt = torch.load(ckpt_path + '/sdfstudio_models' + f'/step-{step:09d}.ckpt', map_location=device)\n",
    "model_dict = {}\n",
    "for key in ckpt['pipeline'].keys():\n",
    "    if key.startswith('_model.'):\n",
    "        model_dict[key[7:]] = ckpt['pipeline'][key]\n",
    "\n",
    "# load yaml checkpoint config\n",
    "config_path = Path(ckpt_path) / 'config.yml'\n",
    "config = yaml.load(config_path.open(), Loader=yaml.Loader)\n",
    "\n",
    "pipeline_config = config.pipeline\n",
    "pipeline_config.datamanager.dataparser.scene = 'lk2'\n",
    "pipeline_config.datamanager.dataparser.use_session_data = False\n",
    "\n",
    "# if illumination_sampler_random_rotation not in pipeline.config.model add it and set to false\n",
    "try:\n",
    "    pipeline_config.model.illumination_sampler_random_rotation\n",
    "except AttributeError:\n",
    "    pipeline_config.model.illumination_sampler_random_rotation = True\n",
    "try:\n",
    "    pipeline_config.model.illumination_sample_remove_lower_hemisphere\n",
    "except AttributeError:\n",
    "    pipeline_config.model.illumination_sample_remove_lower_hemisphere = True\n",
    "\n",
    "datamanager: VanillaDataManager = pipeline_config.datamanager.setup(\n",
    "    device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank, \n",
    ")\n",
    "datamanager.to(device)\n",
    "# includes num_eval_data as needed for reni latent code fitting.\n",
    "model = pipeline_config.model.setup(\n",
    "    scene_box=datamanager.train_dataset.scene_box,\n",
    "    num_train_data=len(datamanager.train_dataset),\n",
    "    num_eval_data=len(datamanager.eval_dataset),\n",
    "    metadata=datamanager.train_dataset.metadata,\n",
    "    world_size=world_size,\n",
    "    local_rank=local_rank,\n",
    "    eval_latent_optimisation_source=pipeline_config.eval_latent_optimisation_source,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()\n",
    "\n",
    "image_idx_original = 3\n",
    "camera_ray_bundle_original, batch_original = datamanager.eval_dataloader.get_data_from_image_idx(image_idx_original)\n",
    "if isinstance(batch_original[\"image\"], BasicImages):\n",
    "    batch_original[\"image\"] = batch_original[\"image\"].images[0]\n",
    "    camera_ray_bundle_original = camera_ray_bundle_original.reshape((*batch_original[\"image\"].shape[:-1], 1))\n",
    "\n",
    "True # printing to hide long cell output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
