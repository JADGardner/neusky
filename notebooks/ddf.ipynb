{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Distance Function Experiments\n",
    "This notebooks contains experiments in using a directional distance field for visibility in RENI-NeuS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set visible devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Variable resolution, using variable_res_collate\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Variable resolution, using variable_res_collate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m160\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4872dd9c0b294205b501064c13ec5f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m5\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a04b7c2ae24d3ba83a6e6017bf8dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/\")\n",
    "import sys\n",
    "sys.path.append(\"/workspace/reni_neus\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.cameras.rays import RayBundle\n",
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "from nerfstudio.field_components.encodings import SHEncoding, NeRFEncoding\n",
    "import tinycudann as tcnn\n",
    "\n",
    "from reni_neus.reni_neus_model import RENINeuSFactoModelConfig, RENINeuSFactoModel\n",
    "from reni_neus.utils.utils import get_directions, get_sineweight\n",
    "from reni_neus.illumination_fields.reni_field import RENIField\n",
    "from reni_neus.data.reni_neus_datamanager import RENINeuSDataManagerConfig, RENINeuSDataManager\n",
    "from reni_neus.reni_neus_config import RENINeuS as RENINeuSMethodSpecification\n",
    "\n",
    "def make_ray_bundle_clone(ray_bundle):\n",
    "    metadata_copy = {}\n",
    "    for key, value in ray_bundle.metadata.items():\n",
    "        metadata_copy[key] = value.detach().clone()\n",
    "\n",
    "    new_ray_bundle = RayBundle(\n",
    "      origins=ray_bundle.origins.detach().clone(),\n",
    "      directions=ray_bundle.directions.detach().clone(),\n",
    "      pixel_area=ray_bundle.pixel_area.detach().clone(),\n",
    "      metadata=metadata_copy,\n",
    "      camera_indices=ray_bundle.camera_indices.detach().clone(),\n",
    "      nears=ray_bundle.nears.detach().clone() if ray_bundle.nears is not None else None,\n",
    "      fars=ray_bundle.fars.detach().clone() if ray_bundle.fars is not None else None,\n",
    "    )\n",
    "    return new_ray_bundle\n",
    "\n",
    "def make_batch_clone(batch):\n",
    "    new_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            new_batch[key] = value.detach().clone()\n",
    "        else:\n",
    "            new_batch[key] = value\n",
    "    return new_batch       \n",
    "\n",
    "def sRGB(imgs):\n",
    "    # Add batch dimension if necessary\n",
    "    if imgs.ndim == 3:\n",
    "        imgs = imgs.unsqueeze(0)\n",
    "    \n",
    "    # Calculate the 98th percentile for each image\n",
    "    q = torch.quantile(imgs.view(imgs.size(0), -1), 0.98, dim=1)\n",
    "    \n",
    "    # Normalize images by their 98th percentile\n",
    "    imgs = imgs / q.view(-1, 1, 1, 1)\n",
    "    \n",
    "    # Clamp the pixel values between 0.0 and 1.0\n",
    "    imgs = torch.clamp(imgs, 0.0, 1.0)\n",
    "    \n",
    "    # Convert linear RGB to sRGB using the sRGB conversion formula\n",
    "    mask = imgs <= 0.0031308\n",
    "    imgs_sRGB = imgs = torch.where(\n",
    "        imgs <= 0.0031308,\n",
    "        12.92 * imgs,\n",
    "        1.055 * torch.pow(torch.abs(imgs), 1 / 2.4) - 0.055,\n",
    "    )\n",
    "    return imgs_sRGB\n",
    "\n",
    "def rotation_matrix(axis, angle):\n",
    "    \"\"\"\n",
    "    Return 3D rotation matrix for rotating around the given axis by the given angle.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(angle / 2.0)\n",
    "    b, c, d = -axis * np.sin(angle / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "\n",
    "# setup config\n",
    "test_mode = 'val'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "datamanager: RENINeuSDataManager = RENINeuSMethodSpecification.config.pipeline.datamanager.setup(\n",
    "    device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank, \n",
    ")\n",
    "datamanager.to(device)\n",
    "\n",
    "model = RENINeuSMethodSpecification.config.pipeline.model.setup(\n",
    "    scene_box=datamanager.train_dataset.scene_box,\n",
    "    num_train_data=len(datamanager.train_dataset),\n",
    "    num_eval_data=len(datamanager.eval_dataset),\n",
    "    metadata=datamanager.train_dataset.metadata,\n",
    "    world_size=world_size,\n",
    "    local_rank=local_rank,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "image_idx_original = 0\n",
    "camera_ray_bundle_original, batch_original = datamanager.eval_dataloader.get_data_from_image_idx(image_idx_original)\n",
    "\n",
    "True # printing to hide long cell output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_ray_bundle = make_ray_bundle_clone(camera_ray_bundle_original)\n",
    "batch = make_batch_clone(batch_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1543090496 is out of bounds for dimension 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# plot batch['mask'] and batch['image] on same figure using matplotlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ray_bundle, batch \u001b[39m=\u001b[39m datamanager\u001b[39m.\u001b[39;49meval_dataloader\u001b[39m.\u001b[39;49mget_data_from_image_idx(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mimshow(batch[\u001b[39m'\u001b[39m\u001b[39mfg_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m/workspace/nerfstudio/data/utils/dataloaders.py:184\u001b[0m, in \u001b[0;36mEvalDataloader.get_data_from_image_idx\u001b[0;34m(self, image_idx)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_from_image_idx\u001b[39m(\u001b[39mself\u001b[39m, image_idx: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[RayBundle, Dict]:\n\u001b[1;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the data for a specific image index.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m        image_idx: Camera image index\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     ray_bundle \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcameras\u001b[39m.\u001b[39;49mgenerate_rays(camera_indices\u001b[39m=\u001b[39;49mimage_idx, keep_shape\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dataset[image_idx]\n\u001b[1;32m    186\u001b[0m     batch \u001b[39m=\u001b[39m get_dict_to_torch(batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, exclude\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspace/nerfstudio/cameras/cameras.py:406\u001b[0m, in \u001b[0;36mCameras.generate_rays\u001b[0;34m(self, camera_indices, coords, camera_opt_to_camera, distortion_params_delta, keep_shape, disable_distortion, aabb_box)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39m# If keep_shape is True, then we need to make sure that the camera indices in question\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[39m# are all the same height and width and can actually be batched while maintaining the image\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39m# shape\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m keep_shape \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mall(cameras\u001b[39m.\u001b[39mheight[camera_indices] \u001b[39m==\u001b[39m cameras\u001b[39m.\u001b[39;49mheight[camera_indices[\u001b[39m0\u001b[39;49m]]) \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mall(\n\u001b[1;32m    407\u001b[0m         cameras\u001b[39m.\u001b[39mwidth[camera_indices] \u001b[39m==\u001b[39m cameras\u001b[39m.\u001b[39mwidth[camera_indices[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    408\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCan only keep shape if all cameras have the same height and width\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[39m# If the cameras don't all have same height / width, if coords is not none, we will need to generate\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39m# a flat list of coords for each camera and then concatenate otherwise our rays will be jagged.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m# Camera indices, camera_opt, and distortion will also need to be broadcasted accordingly which is non-trivial\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m cameras\u001b[39m.\u001b[39mis_jagged \u001b[39mand\u001b[39;00m coords \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (keep_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m keep_shape \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m):\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1543090496 is out of bounds for dimension 0 with size 5"
     ]
    }
   ],
   "source": [
    "# plot batch['mask'] and batch['image] on same figure using matplotlib\n",
    "ray_bundle, batch = datamanager.eval_dataloader.get_data_from_image_idx(0)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(batch['fg_mask'].cpu().numpy())\n",
    "ax[1].imshow(batch['image'].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['fg_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
