{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "import pyexr\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.cameras.rays import RayBundle, RaySamples, Frustums\n",
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "from nerfstudio.field_components.encodings import SHEncoding, NeRFEncoding\n",
    "from nerfstudio.viewer.server import viewer_utils\n",
    "import tinycudann as tcnn\n",
    "\n",
    "from reni_neus.models.reni_neus_model import RENINeuSFactoModelConfig, RENINeuSFactoModel\n",
    "from reni_neus.utils.utils import look_at_target, random_points_on_unit_sphere\n",
    "from reni_neus.data.datamanagers.reni_neus_datamanager import RENINeuSDataManagerConfig, RENINeuSDataManager\n",
    "from reni_neus.configs.ddf_config import DirectionalDistanceField\n",
    "from reni_neus.configs.reni_neus_config import RENINeuS\n",
    "from reni_neus.utils.utils import find_nerfstudio_project_root, rot_z\n",
    "\n",
    "from reni.illumination_fields.environment_map_field import EnvironmentMapFieldConfig\n",
    "\n",
    "project_root = find_nerfstudio_project_root(Path(os.getcwd()))\n",
    "# set current working directory to nerfstudio project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "# setup config\n",
    "test_mode = 'test'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "scene = 'site1'\n",
    "\n",
    "reni_neus_config = RENINeuS\n",
    "reni_neus_ckpt_path = '/workspace/reni_neus/models/site1' # model without vis\n",
    "step = 100000\n",
    "reni_neus_ckpt = torch.load(reni_neus_ckpt_path + '/nerfstudio_models' + f'/step-{step:09d}.ckpt', map_location=device)\n",
    "reni_neus_model_dict = {}\n",
    "for key in reni_neus_ckpt['pipeline'].keys():\n",
    "    if key.startswith('_model.'):\n",
    "        reni_neus_model_dict[key[7:]] = reni_neus_ckpt['pipeline'][key]\n",
    "\n",
    "if scene == 'site1':\n",
    "    reni_neus_config.config.pipeline.datamanager.dataparser.session_holdout_indices=[0, 0, 0, 0, 0]\n",
    "elif scene == 'site2':\n",
    "    reni_neus_config.config.pipeline.datamanager.dataparser.session_holdout_indices=[1, 2, 2, 7, 9]\n",
    "elif scene == 'site3':\n",
    "    reni_neus_config.config.pipeline.datamanager.dataparser.session_holdout_indices=[0, 6, 6, 2, 11]\n",
    "elif scene == 'stjacob':\n",
    "    reni_neus_config.config.pipeline.datamanager.dataparser.session_holdout_indices=[0, 0, 0]\n",
    "\n",
    "datamanager: RENINeuSDataManager = RENINeuS.config.pipeline.datamanager.setup(\n",
    "    device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank, \n",
    ")\n",
    "datamanager.to(device)\n",
    "\n",
    "# instantiate model with config with vis\n",
    "model = RENINeuS.config.pipeline.model.setup(\n",
    "    scene_box=datamanager.train_dataset.scene_box,\n",
    "    num_train_data=len(datamanager.train_dataset),\n",
    "    num_val_data=datamanager.num_val,\n",
    "    num_test_data=datamanager.num_test,\n",
    "    test_mode=test_mode,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(reni_neus_model_dict)\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:28:42] </span>Saving checkpoints to: outputs/ddf/ddf/<span style=\"font-weight: bold\">{</span>timestamp<span style=\"font-weight: bold\">}</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nerfstudio_models</span>                           <a href=\"file:///workspace/nerfstudio/engine/trainer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">trainer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///workspace/nerfstudio/engine/trainer.py#134\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[14:28:42]\u001b[0m\u001b[2;36m \u001b[0mSaving checkpoints to: outputs/ddf/ddf/\u001b[1m{\u001b[0mtimestamp\u001b[1m}\u001b[0m\u001b[35m/\u001b[0m\u001b[95mnerfstudio_models\u001b[0m                           \u001b]8;id=646763;file:///workspace/nerfstudio/engine/trainer.py\u001b\\\u001b[2mtrainer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=336061;file:///workspace/nerfstudio/engine/trainer.py#134\u001b\\\u001b[2m134\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">160</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m160\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d6b7d710b54e95a6ef08b4b74f0eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m95\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d09ba66cce44de8ab6c3a39320157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">95</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m95\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dcc0d7f40140048c6015b0d0adf3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────── <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Viewer</span> ───────────────────────────────────────────╮\n",
       "│        ╷                                                                                     │\n",
       "│   HTTP │ <a href=\"https://viewer.nerf.studio/versions/23-05-15-1/?websocket_url=ws://localhost:7007\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080\">https://viewer.nerf.studio/versions/23-05-15-1/?websocket_url=ws://localhost:7007</span></a>   │\n",
       "│        ╵                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────── \u001b[1;33mViewer\u001b[0m ───────────────────────────────────────────╮\n",
       "│        ╷                                                                                     │\n",
       "│   HTTP │ \u001b]8;id=734331;https://viewer.nerf.studio/versions/23-05-15-1/?websocket_url=ws://localhost:7007\u001b\\\u001b[34mhttps://viewer.nerf.studio/versions/23-05-15-1/?websocket_url=ws://localhost:7007\u001b[0m\u001b]8;;\u001b\\   │\n",
       "│        ╵                                                                                     │\n",
       "╰──────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>NOTE<span style=\"font-weight: bold\">]</span> Not running eval iterations since only viewer is enabled.\n",
       "Use <span style=\"color: #808000; text-decoration-color: #808000\">--vis </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">{</span><span style=\"color: #808000; text-decoration-color: #808000\">wandb, tensorboard, viewer+wandb, viewer+tensorboard</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">}</span> to run with eval.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mNOTE\u001b[1m]\u001b[0m Not running eval iterations since only viewer is enabled.\n",
       "Use \u001b[33m--vis \u001b[0m\u001b[1;33m{\u001b[0m\u001b[33mwandb, tensorboard, viewer+wandb, viewer+tensorboard\u001b[0m\u001b[1;33m}\u001b[0m to run with eval.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">No Nerfstudio checkpoint to load, so training from scratch.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "No Nerfstudio checkpoint to load, so training from scratch.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Disabled comet/tensorboard/wandb event writers</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mDisabled comet/tensorboard/wandb event writers\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from torch.utils.data import Dataset\n",
    "import pyexr\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.cameras.rays import RayBundle, RaySamples, Frustums\n",
    "from nerfstudio.utils.colormaps import apply_depth_colormap\n",
    "from nerfstudio.field_components.encodings import SHEncoding, NeRFEncoding\n",
    "from nerfstudio.viewer.server import viewer_utils\n",
    "import tinycudann as tcnn\n",
    "\n",
    "from reni_neus.models.reni_neus_model import RENINeuSFactoModelConfig, RENINeuSFactoModel\n",
    "from reni_neus.utils.utils import look_at_target, random_points_on_unit_sphere\n",
    "from reni_neus.data.datamanagers.reni_neus_datamanager import RENINeuSDataManagerConfig, RENINeuSDataManager\n",
    "from reni_neus.configs.ddf_config import DirectionalDistanceField\n",
    "from reni_neus.configs.reni_neus_config import RENINeuS\n",
    "from reni_neus.utils.utils import find_nerfstudio_project_root, rot_z\n",
    "\n",
    "from reni.illumination_fields.environment_map_field import EnvironmentMapFieldConfig\n",
    "\n",
    "project_root = find_nerfstudio_project_root(Path(os.getcwd()))\n",
    "# set current working directory to nerfstudio project root\n",
    "os.chdir(project_root)\n",
    "\n",
    "# setup config\n",
    "test_mode = 'test'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "scene = 'site1'\n",
    "\n",
    "ddf_config = DirectionalDistanceField\n",
    "ddf_config.config.pipeline.reni_neus_ckpt_path = Path('/workspace/reni_neus/models/site1')\n",
    "\n",
    "trainer = ddf_config.config.setup(local_rank=local_rank, world_size=world_size)\n",
    "trainer.setup(test_mode=test_mode)\n",
    "pipeline = trainer.pipeline\n",
    "datamanager = pipeline.datamanager\n",
    "model = pipeline.model\n",
    "model = model.eval()\n",
    "\n",
    "reni_neus_ckpt_path = '/workspace/reni_neus/models/site1' # model without vis\n",
    "step = 100000\n",
    "reni_neus_ckpt = torch.load(reni_neus_ckpt_path + '/nerfstudio_models' + f'/step-{step:09d}.ckpt', map_location=device)\n",
    "ddf_model_dict = {}\n",
    "for key in reni_neus_ckpt['pipeline'].keys():\n",
    "    if key.startswith('_model.visibility_field.'):\n",
    "        ddf_model_dict[key[24:]] = reni_neus_ckpt['pipeline'][key]\n",
    "\n",
    "\n",
    "model.load_state_dict(ddf_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering frame_idx: 0\n",
      "Rendering frame_idx: 1\n",
      "Rendering frame_idx: 2\n",
      "Rendering frame_idx: 3\n",
      "Rendering frame_idx: 4\n",
      "Rendering frame_idx: 5\n",
      "Rendering frame_idx: 6\n",
      "Rendering frame_idx: 7\n",
      "Rendering frame_idx: 8\n",
      "Rendering frame_idx: 9\n",
      "Rendering frame_idx: 10\n",
      "Rendering frame_idx: 11\n",
      "Rendering frame_idx: 12\n",
      "Rendering frame_idx: 13\n",
      "Rendering frame_idx: 14\n",
      "Rendering frame_idx: 15\n",
      "Rendering frame_idx: 16\n",
      "Rendering frame_idx: 17\n",
      "Rendering frame_idx: 18\n",
      "Rendering frame_idx: 19\n",
      "Rendering frame_idx: 20\n",
      "Rendering frame_idx: 21\n",
      "Rendering frame_idx: 22\n",
      "Rendering frame_idx: 23\n",
      "Rendering frame_idx: 24\n",
      "Rendering frame_idx: 25\n",
      "Rendering frame_idx: 26\n",
      "Rendering frame_idx: 27\n",
      "Rendering frame_idx: 28\n",
      "Rendering frame_idx: 29\n",
      "Rendering frame_idx: 30\n",
      "Rendering frame_idx: 31\n",
      "Rendering frame_idx: 32\n",
      "Rendering frame_idx: 33\n",
      "Rendering frame_idx: 34\n",
      "Rendering frame_idx: 35\n",
      "Rendering frame_idx: 36\n",
      "Rendering frame_idx: 37\n",
      "Rendering frame_idx: 38\n",
      "Rendering frame_idx: 39\n",
      "Rendering frame_idx: 40\n",
      "Rendering frame_idx: 41\n",
      "Rendering frame_idx: 42\n",
      "Rendering frame_idx: 43\n",
      "Rendering frame_idx: 44\n",
      "Rendering frame_idx: 45\n",
      "Rendering frame_idx: 46\n",
      "Rendering frame_idx: 47\n",
      "Rendering frame_idx: 48\n",
      "Rendering frame_idx: 49\n",
      "Rendering frame_idx: 50\n",
      "Rendering frame_idx: 51\n",
      "Rendering frame_idx: 52\n",
      "Rendering frame_idx: 53\n",
      "Rendering frame_idx: 54\n",
      "Rendering frame_idx: 55\n",
      "Rendering frame_idx: 56\n",
      "Rendering frame_idx: 57\n",
      "Rendering frame_idx: 58\n",
      "Rendering frame_idx: 59\n",
      "Rendering frame_idx: 60\n",
      "Rendering frame_idx: 61\n",
      "Rendering frame_idx: 62\n",
      "Rendering frame_idx: 63\n",
      "Rendering frame_idx: 64\n",
      "Rendering frame_idx: 65\n",
      "Rendering frame_idx: 66\n",
      "Rendering frame_idx: 67\n",
      "Rendering frame_idx: 68\n",
      "Rendering frame_idx: 69\n",
      "Rendering frame_idx: 70\n",
      "Rendering frame_idx: 71\n",
      "Rendering frame_idx: 72\n",
      "Rendering frame_idx: 73\n",
      "Rendering frame_idx: 74\n",
      "Rendering frame_idx: 75\n",
      "Rendering frame_idx: 76\n",
      "Rendering frame_idx: 77\n",
      "Rendering frame_idx: 78\n",
      "Rendering frame_idx: 79\n",
      "Rendering frame_idx: 80\n",
      "Rendering frame_idx: 81\n",
      "Rendering frame_idx: 82\n",
      "Rendering frame_idx: 83\n",
      "Rendering frame_idx: 84\n",
      "Rendering frame_idx: 85\n",
      "Rendering frame_idx: 86\n",
      "Rendering frame_idx: 87\n",
      "Rendering frame_idx: 88\n",
      "Rendering frame_idx: 89\n",
      "Rendering frame_idx: 90\n",
      "Rendering frame_idx: 91\n",
      "Rendering frame_idx: 92\n",
      "Rendering frame_idx: 93\n",
      "Rendering frame_idx: 94\n",
      "Rendering frame_idx: 95\n",
      "Rendering frame_idx: 96\n",
      "Rendering frame_idx: 97\n",
      "Rendering frame_idx: 98\n",
      "Rendering frame_idx: 99\n",
      "Rendering frame_idx: 100\n",
      "Rendering frame_idx: 101\n",
      "Rendering frame_idx: 102\n",
      "Rendering frame_idx: 103\n",
      "Rendering frame_idx: 104\n",
      "Rendering frame_idx: 105\n",
      "Rendering frame_idx: 106\n",
      "Rendering frame_idx: 107\n",
      "Rendering frame_idx: 108\n",
      "Rendering frame_idx: 109\n",
      "Rendering frame_idx: 110\n",
      "Rendering frame_idx: 111\n",
      "Rendering frame_idx: 112\n",
      "Rendering frame_idx: 113\n",
      "Rendering frame_idx: 114\n",
      "Rendering frame_idx: 115\n",
      "Rendering frame_idx: 116\n",
      "Rendering frame_idx: 117\n",
      "Rendering frame_idx: 118\n",
      "Rendering frame_idx: 119\n",
      "Rendering frame_idx: 120\n",
      "Rendering frame_idx: 121\n",
      "Rendering frame_idx: 122\n",
      "Rendering frame_idx: 123\n",
      "Rendering frame_idx: 124\n",
      "Rendering frame_idx: 125\n",
      "Rendering frame_idx: 126\n",
      "Rendering frame_idx: 127\n",
      "Rendering frame_idx: 128\n",
      "Rendering frame_idx: 129\n",
      "Rendering frame_idx: 130\n",
      "Rendering frame_idx: 131\n",
      "Rendering frame_idx: 132\n",
      "Rendering frame_idx: 133\n",
      "Rendering frame_idx: 134\n",
      "Rendering frame_idx: 135\n",
      "Rendering frame_idx: 136\n",
      "Rendering frame_idx: 137\n",
      "Rendering frame_idx: 138\n",
      "Rendering frame_idx: 139\n",
      "Rendering frame_idx: 140\n",
      "Rendering frame_idx: 141\n",
      "Rendering frame_idx: 142\n",
      "Rendering frame_idx: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1920, 1080) to (1920, 1088) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation saved at /workspace/reni_neus/publication/animations/ddf_20231123_144608/animation_render.mp4\n"
     ]
    }
   ],
   "source": [
    "from nerfstudio.utils.io import load_from_json\n",
    "from nerfstudio.cameras.cameras import Cameras, CameraType\n",
    "from datetime import datetime\n",
    "import math\n",
    "from nerfstudio.utils import colormaps\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "camera_poses_path = f'/workspace/reni_neus/publication/ddf_camera_path.json'\n",
    "meta = load_from_json(Path(camera_poses_path))\n",
    "fps = meta['fps']\n",
    "\n",
    "# create folder in /workspace/reni_neus/publication/animations/{scene}_datetime\n",
    "# save all rendered images in this folder\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_path = f'/workspace/reni_neus/publication/animations/ddf_{datetime_str}'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "render_height = meta['render_height']\n",
    "render_width = meta['render_width']\n",
    "render_height = 1080 # 1080, 144\n",
    "render_width = 1920 # 1920, 256\n",
    "cx = render_width / 2.0\n",
    "cy = render_height / 2.0\n",
    "fov = meta['keyframes'][0]['fov']\n",
    "aspect = render_width / render_height\n",
    "fx = render_width / (2 * math.tan(math.radians(fov) / 2))\n",
    "fy = fx\n",
    "c2w = torch.eye(4)[:3, :4]\n",
    "\n",
    "camera = Cameras(camera_to_worlds=c2w,\n",
    "                 fy=fy,\n",
    "                 fx=fx,\n",
    "                 cx=cx,\n",
    "                 cy=cy,\n",
    "                 camera_type=CameraType.PERSPECTIVE)\n",
    "\n",
    "base_ray_bundle = datamanager.train_dataset.cameras[0].generate_rays(0)\n",
    "base_ray_bundle = base_ray_bundle.to(device)\n",
    "\n",
    "\n",
    "def save_model_output(model_output, frame_num, path):\n",
    "    rendered_image = model_output['expected_termination_dist']\n",
    "\n",
    "    # def to_rgb_tensor(gray_tensor, cmap=\"viridis\"):\n",
    "    #         # Ensure the tensor is in the range [0, 1]\n",
    "    #         normalized_tensor = (gray_tensor - 0.0) / (2.0 - 0.0)\n",
    "\n",
    "    #         # Convert to numpy and use colormap to get RGB values\n",
    "    #         cmapped = cm.get_cmap(cmap)(normalized_tensor.cpu().numpy())\n",
    "\n",
    "    #         # Convert back to tensor and take only RGB channels (discard alpha)\n",
    "    #         rgb_tensor = torch.tensor(cmapped[..., :3])\n",
    "\n",
    "    #         return rgb_tensor\n",
    "\n",
    "    # rendered_image = to_rgb_tensor(rendered_image).cpu().detach().numpy()\n",
    "\n",
    "    rendered_image = colormaps.apply_depth_colormap(rendered_image, near_plane=0.0, far_plane=2.0).cpu().detach().numpy()\n",
    "\n",
    "    plt.imsave(f'{path}/frame_{str(frame_num).zfill(3)}_render.png', rendered_image)\n",
    "\n",
    "# Add a parameter for an optional output folder\n",
    "def process_scene(camera_poses_path):\n",
    "    meta = load_from_json(Path(camera_poses_path))\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    for frame_idx, frame in enumerate(meta['camera_path']):       \n",
    "        camera_to_world = torch.from_numpy(np.array(frame['camera_to_world']).reshape((4, 4))).to(torch.float32)\n",
    "        # ensure the position is normalised onto the unit sphere\n",
    "        camera_to_world[:3, 3] = camera_to_world[:3, 3] / torch.norm(camera_to_world[:3, 3])\n",
    "        camera.camera_to_worlds = camera_to_world[:3, :4]\n",
    "        ray_bundle = camera.generate_rays(0)\n",
    "        ray_bundle = ray_bundle.to(device)\n",
    "        ray_bundle.camera_indices = torch.ones_like(ray_bundle.camera_indices)\n",
    "        print(f'Rendering frame_idx: {frame_idx}')\n",
    "        model_output = model.get_outputs_for_camera_ray_bundle(camera_ray_bundle=ray_bundle)\n",
    "        save_model_output(model_output, frame_idx, folder_path)\n",
    "\n",
    "# Call with an optional output folder\n",
    "process_scene(camera_poses_path)\n",
    "\n",
    "def create_animation(folder, image_type, fps, format='gif'):\n",
    "    \"\"\"\n",
    "    Creates an animation from images of a specific type in the given folder.\n",
    "\n",
    "    :param folder: Path to the folder containing the images.\n",
    "    :param image_type: Type of the image (e.g., 'render', 'albedo', 'normal').\n",
    "    :param fps: Frames per second for the output animation.\n",
    "    :param format: Output format of the animation ('gif' or 'mp4').\n",
    "    \"\"\"\n",
    "    # List all files in the folder\n",
    "    files = sorted([f for f in os.listdir(folder) if f.endswith(f\"{image_type}.png\")])\n",
    "\n",
    "    if not files:\n",
    "        print(\"No images found for the specified type.\")\n",
    "        return\n",
    "\n",
    "    # Read images and store them in a list\n",
    "    images = []\n",
    "    for file in files:\n",
    "        img_path = os.path.join(folder, file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            # Convert from BGR to RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f\"Failed to read image: {img_path}\")\n",
    "\n",
    "    # Create animation\n",
    "    output_path = os.path.join(folder, f\"animation_{image_type}.{format}\")\n",
    "    if format == 'gif':\n",
    "        imageio.mimsave(output_path, images, fps=fps)\n",
    "    elif format == 'mp4':\n",
    "        writer = imageio.get_writer(output_path, fps=fps, codec='libx264')\n",
    "        for img in images:\n",
    "            writer.append_data(img)\n",
    "        writer.close()\n",
    "    else:\n",
    "        print(\"Unsupported format. Please choose 'gif' or 'mp4'.\")\n",
    "\n",
    "    print(f\"Animation saved at {output_path}\")\n",
    "\n",
    "create_animation(folder_path, 'render', 24, 'mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
